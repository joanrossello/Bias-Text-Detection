{"cells":[{"cell_type":"code","source":["# choose dataset from 'NPOV', 'WNC', 'CrowS-Pairs', 'Stereo', 'Mixed'\n","dataset = 'Stereo'"],"metadata":{"id":"GgKxTZgTxrfw","executionInfo":{"status":"ok","timestamp":1663375417949,"user_tz":-60,"elapsed":8,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["# Imports and Set-up"],"metadata":{"id":"Se4vnWpASj_k"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"I9klrk6JieAp","executionInfo":{"status":"ok","timestamp":1663375106258,"user_tz":-60,"elapsed":3471,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","from torch.utils.data.dataloader import default_collate\n","import numpy as np\n","import math\n","import pandas as pd\n","import shutil\n","from string import punctuation\n","from collections import Counter \n","import os\n","\n","# Evaluation\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import seaborn as sns"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22158,"status":"ok","timestamp":1663375128411,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"5qF4exWBntFk","outputId":"dff18079-42ea-4e2a-f584-26a394b604ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"kp70KMA7nvyS","executionInfo":{"status":"ok","timestamp":1663375128411,"user_tz":-60,"elapsed":7,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"markdown","source":["# Dataset and Dataloader"],"metadata":{"id":"76ifE2bKSh8N"}},{"cell_type":"code","source":["# import dataset\n","source_folder = \"/content/drive/MyDrive/Colab Notebooks/Amplifi Project/Data/Processed Datasets/\"\n","df = pd.read_csv(source_folder + dataset + '.csv', delimiter=',')"],"metadata":{"id":"ILp7et5RexAA","executionInfo":{"status":"ok","timestamp":1663375420478,"user_tz":-60,"elapsed":311,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","execution_count":34,"metadata":{"id":"KH02gdv8ieAs","executionInfo":{"status":"ok","timestamp":1663375420904,"user_tz":-60,"elapsed":6,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["texts = df['text'].to_numpy().ravel('F').tolist() # turn into a list\n","labels = df['label'].to_numpy().ravel('F').tolist() # turn into a list"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"AfZZy4U-ieAt","executionInfo":{"status":"ok","timestamp":1663375420904,"user_tz":-60,"elapsed":6,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["# Remove punctuation and get the vocabulary of the dataset (i.e. all the words in the sentences)\n","\n","all_texts=list()\n","for text in texts:\n","    text = text.lower()\n","    text = \"\".join([ch for ch in text if ch not in punctuation])\n","    all_texts.append(text)\n","all_text = \" \".join(all_texts) # list of the texts in \"texts\" but without punctuation\n","all_words = all_text.split() # all words in \"all_text\" with repetitions"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"JTWIUyW8ieAt","executionInfo":{"status":"ok","timestamp":1663375421231,"user_tz":-60,"elapsed":332,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["# Count all the words using Counter Method\n","\n","count_words = Counter(all_words)\n","total_words=len(all_words)\n","sorted_words=count_words.most_common(total_words)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1663375421231,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"00tz9G30ieAu","outputId":"036c067a-31dd-4355-d7c7-fc0f8e084998"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('the', 1641),\n"," ('a', 946),\n"," ('to', 734),\n"," ('was', 599),\n"," ('and', 569),\n"," ('is', 551),\n"," ('are', 461),\n"," ('in', 448),\n"," ('of', 443),\n"," ('he', 418)]"]},"metadata":{},"execution_count":37}],"source":["# Top 10 occurring words\n","\n","sorted_words[:10]"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"E93l1xwAieAv","executionInfo":{"status":"ok","timestamp":1663375421231,"user_tz":-60,"elapsed":14,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["# Create a dictionary and give each work an ID which is equal to its rank of occurrence\n","# We reserve ID 0 for padding\n","\n","vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"RpKHBufcieAv","executionInfo":{"status":"ok","timestamp":1663375421232,"user_tz":-60,"elapsed":14,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["#Â We express the texts as encodings (i.e. the words are replaced with their respective IDs)\n","\n","encoded_texts = list()\n","for text in all_texts:\n","  encoded_text = list()\n","  for word in text.split():\n","    if word not in vocab_to_int.keys():\n","      #if word is not available in vocab_to_int put 0 in that place\n","      encoded_text.append(0)\n","    else:\n","      encoded_text.append(vocab_to_int[word])\n","  encoded_texts.append(encoded_text)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"1LN_8mcOieAx","executionInfo":{"status":"ok","timestamp":1663375421232,"user_tz":-60,"elapsed":14,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["# make all encoded texts of the same length (i.e. add pre-padding)\n","\n","sequence_length = 0\n","\n","for text in encoded_texts:\n","    if len(text) > sequence_length:\n","        sequence_length = len(text)\n","\n","features=np.zeros((len(encoded_texts), sequence_length), dtype=int)\n","for i, text in enumerate(encoded_texts):\n","  text_len=len(text)\n","  if (text_len<=sequence_length):\n","    zeros=list(np.zeros(sequence_length-text_len))\n","    new=zeros+text # note that we add the padding at the beginning instead of at the end\n","  else:\n","    new=text[:sequence_length]\n","  \n","  features[i,:]=np.array(new)"]},{"cell_type":"code","source":["features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fg3LMyJ_GSj","executionInfo":{"status":"ok","timestamp":1663375421232,"user_tz":-60,"elapsed":14,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}},"outputId":"bf214f37-8730-4ea3-e39f-d85544d29403"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[   0,    0,    0, ...,  538,   39,   15],\n","       [   0,    0,    0, ...,   19,    1, 1271],\n","       [   0,    0,    0, ...,  617,    3,   59],\n","       ...,\n","       [   0,    0,    0, ...,   42, 4557,  355],\n","       [   0,    0,    0, ..., 4560,   25,  739],\n","       [   0,    0,    0, ..., 1984,    1,  169]])"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1663375421233,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"uRyPoFwCieAx","outputId":"efef3e2c-483e-47e2-9c89-d7f197b8ef9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["1776 222 223\n"]}],"source":["# Split data into training/validation/testing --> 80/10/10\n","\n","train_x=features[:int(0.8*len(features))]\n","train_y=labels[:int(0.8*len(features))]\n","valid_x=features[int(0.8*len(features)):int(0.9*len(features))]\n","valid_y=labels[int(0.8*len(features)):int(0.9*len(features))]\n","test_x=features[int(0.9*len(features)):]\n","test_y=labels[int(0.9*len(features)):]\n","print(len(train_y), len(valid_y), len(test_y))"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"gfJxwJIHieAy","executionInfo":{"status":"ok","timestamp":1663375421234,"user_tz":-60,"elapsed":14,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["#create Tensor Dataset\n","train_data=TensorDataset(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n","valid_data=TensorDataset(torch.FloatTensor(valid_x), torch.FloatTensor(valid_y))\n","test_data=TensorDataset(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\n","\n","#dataloader\n","batch_size=50\n","train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n","test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1663375421235,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"77hoWJibieAz","outputId":"0d14b1b1-eed8-47f9-8954-70e2c0f64358"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample input size:  torch.Size([50, 40])\n","Sample input: \n"," tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6500e+02, 1.0000e+00,\n","         8.1500e+02],\n","        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.0000e+00, 1.0000e+00,\n","         2.1800e+02],\n","        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0000e+00, 1.0600e+02,\n","         5.0300e+02],\n","        ...,\n","        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0800e+02, 6.0000e+00,\n","         2.2630e+03],\n","        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.1400e+03, 2.6000e+01,\n","         3.1410e+03],\n","        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.0420e+03, 8.0000e+00,\n","         1.8370e+03]])\n","Sample label size:  torch.Size([50])\n","Sample label: \n"," tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n","        0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n","        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.])\n"]}],"source":["# Analyse the DataLoader --> obtain one batch of training data\n","dataiter = iter(train_loader)\n","sample_x, sample_y = dataiter.next()\n","print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n","print('Sample input: \\n', sample_x)\n","print('Sample label size: ', sample_y.size()) # batch_size\n","print('Sample label: \\n', sample_y)"]},{"cell_type":"markdown","source":["# Model Architecture"],"metadata":{"id":"PCI-NKijSdn8"}},{"cell_type":"code","execution_count":45,"metadata":{"id":"bLBB4S17ieAz","executionInfo":{"status":"ok","timestamp":1663375423799,"user_tz":-60,"elapsed":408,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[],"source":["#Â Create architecture\n","\n","class SentimentalLSTM(nn.Module):\n","    \"\"\"\n","    The RNN model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.3):\n","        \"\"\"\n","        Initialize the model by setting up the layers\n","        \"\"\"\n","        super().__init__()\n","        self.output_size=output_size\n","        self.n_layers=n_layers\n","        self.hidden_dim=hidden_dim\n","        \n","        #Embedding and LSTM layers\n","        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","        \n","        #dropout layer\n","        self.dropout=nn.Dropout(0.3)\n","        \n","        #Linear and sigmoid layer\n","        self.fc1=nn.Linear(hidden_dim, output_size)\n","        self.sigmoid=nn.Sigmoid()\n","        \n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","\n","        x = x.to(torch.int64) # added, to solve RuntimeError\n","        # x = torch.tensor(x).to(torch.int64) # added, to solve RuntimeError\n","\n","        batch_size=x.size()\n","        \n","        #Embedding and LSTM output\n","        embedd=self.embedding(x)\n","        lstm_out, hidden=self.lstm(embedd, hidden)\n","        \n","        #stack up the lstm output\n","        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","        #dropout and fully connected layers\n","        out=self.dropout(lstm_out)\n","        out=self.fc1(out)\n","        sig_out=self.sigmoid(out)\n","        \n","        sig_out=sig_out.view(batch_size, -1)\n","        sig_out=sig_out[:, -1]\n","        \n","        return sig_out, hidden\n","    \n","    def init_hidden(self, batch_size):\n","        \"\"\"Initialize Hidden STATE\"\"\"\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden"]},{"cell_type":"markdown","source":["# Model and Hyperparameters"],"metadata":{"id":"RIuvY1rLSZOe"}},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1663375424216,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"4GDxbqdxieA0","outputId":"02ac5b75-ede3-4d9b-b4ae-8c40c4673eac"},"outputs":[{"output_type":"stream","name":"stdout","text":["SentimentalLSTM(\n","  (embedding): Embedding(4561, 300)\n","  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.3)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc1): Linear(in_features=128, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nOutput:\\n SentimentalLSTM(   \\n(embedding): Embedding(74073, 400)   \\n(lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)   (dropout): Dropout(p=0.3)   \\n(fc): Linear(in_features=256, out_features=1, bias=True)   (sigmoid): Sigmoid() )\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}],"source":["# Instantiate the model w/ hyperparams\n","\n","vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n","output_size = 1\n","embedding_dim = 300\n","hidden_dim = 128\n","n_layers = 2\n","bi_directional = False\n","\n","net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","print(net)\n","\n","'''\n","Output:\n"," SentimentalLSTM(   \n","(embedding): Embedding(74073, 400)   \n","(lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)   (dropout): Dropout(p=0.3)   \n","(fc): Linear(in_features=256, out_features=1, bias=True)   (sigmoid): Sigmoid() )\n","'''"]},{"cell_type":"markdown","source":["# Model Checkpoint"],"metadata":{"id":"ieleTj7eSRGC"}},{"cell_type":"code","source":["# Save and Load Functions\n","\n","def save_checkpoint(save_path, model, optimizer, valid_loss):\n","\n","    if save_path == None:\n","        return\n","    \n","    state_dict = {'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict(),\n","                  'valid_loss': valid_loss}\n","    \n","    torch.save(state_dict, save_path)\n","    print(f'Model saved to ==> {save_path}')\n","\n","\n","def load_checkpoint(load_path, model, optimizer):\n","\n","    if load_path==None:\n","        return\n","    \n","    state_dict = torch.load(load_path, map_location=device)\n","    print(f'Model loaded from <== {load_path}')\n","    \n","    model.load_state_dict(state_dict['model_state_dict'])\n","    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n","    \n","    return state_dict['valid_loss']"],"metadata":{"id":"Qpkql09eSUQW","executionInfo":{"status":"ok","timestamp":1663375145364,"user_tz":-60,"elapsed":3,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# dataset = 'NPOV' # load NPOV trained model\n","\n","net.to(device)\n","lr=0.001\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(params =  net.parameters(), lr=lr)\n","\n","destination_folder = \"/content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints\"\n","\n","try:\n","  load_checkpoint(destination_folder + '/lstm_model_' + dataset + '.pt', net, optimizer) # comment this if you wannt to train the model from zero\n","  print('lstm_model_' + dataset + '.pt --> loaded')\n","except:\n","  print('training lstm_model_' + dataset + ' from scratch')\n"],"metadata":{"id":"eXDAlSh_Ngwl","executionInfo":{"status":"ok","timestamp":1663375476102,"user_tz":-60,"elapsed":4528,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c76ec65c-1827-4627-8ed5-a5ff37db84a9"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded from <== /content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints/lstm_model_NPOV.pt\n","training lstm_model_NPOV from scratch\n"]}]},{"cell_type":"markdown","source":["# Train Model"],"metadata":{"id":"lhhGohtKSUkS"}},{"cell_type":"code","source":["if dataset == 'Stereo':\n","  val_loss_min = 0.858123\n","elif dataset == 'NPOV':\n","  val_loss_min = 0.522309\n","elif dataset == 'Mixed':\n","  val_loss_min = np.Inf\n","else:\n","  val_loss_min = np.Inf"],"metadata":{"id":"LRCLLkaY75xd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{"id":"1nzx28XnieA1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663375213527,"user_tz":-60,"elapsed":2939,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}},"outputId":"d982a4a9-3db4-42ef-a7c5-ce8b75d7fc3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss decreased (inf --> 1.129760).  Saving model ...\n","Model saved to ==> /content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints/lstm_model_Stereo.pt\n","Epoch: 1/10... Step: 6... Loss: 0.620617... Val Loss: 0.742576\n","Epoch: 1/10... Step: 12... Loss: 0.562976... Val Loss: 0.871720\n","Epoch: 1/10... Step: 18... Loss: 0.545510... Val Loss: 0.702273\n","Epoch: 1/10... Step: 24... Loss: 0.412801... Val Loss: 0.684113\n","Epoch: 1/10... Step: 30... Loss: 0.566342... Val Loss: 0.765450\n","Epoch: 2/10... Step: 6... Loss: 0.265729... Val Loss: 0.801111\n","Epoch: 2/10... Step: 12... Loss: 0.447123... Val Loss: 0.787933\n","Epoch: 2/10... Step: 18... Loss: 0.229172... Val Loss: 0.852352\n","Epoch: 2/10... Step: 24... Loss: 0.420937... Val Loss: 0.804489\n","Epoch: 2/10... Step: 30... Loss: 0.373012... Val Loss: 0.770287\n","Epoch: 3/10... Step: 6... Loss: 0.277467... Val Loss: 0.875895\n","Epoch: 3/10... Step: 12... Loss: 0.243606... Val Loss: 0.891707\n","Epoch: 3/10... Step: 18... Loss: 0.328635... Val Loss: 1.021258\n","Epoch: 3/10... Step: 24... Loss: 0.242546... Val Loss: 0.911689\n","Epoch: 3/10... Step: 30... Loss: 0.279815... Val Loss: 0.836191\n","Epoch: 4/10... Step: 6... Loss: 0.130451... Val Loss: 1.033656\n","Epoch: 4/10... Step: 12... Loss: 0.110538... Val Loss: 0.976129\n","Epoch: 4/10... Step: 18... Loss: 0.082419... Val Loss: 1.260190\n","Epoch: 4/10... Step: 24... Loss: 0.158554... Val Loss: 1.174912\n","Epoch: 4/10... Step: 30... Loss: 0.160304... Val Loss: 1.212250\n","Epoch: 5/10... Step: 6... Loss: 0.051815... Val Loss: 1.285560\n","Epoch: 5/10... Step: 12... Loss: 0.097040... Val Loss: 1.468102\n","Epoch: 5/10... Step: 18... Loss: 0.134649... Val Loss: 1.433819\n","Epoch: 5/10... Step: 24... Loss: 0.061282... Val Loss: 1.390607\n","Epoch: 5/10... Step: 30... Loss: 0.097967... Val Loss: 1.453903\n","Epoch: 6/10... Step: 6... Loss: 0.077510... Val Loss: 1.566647\n","Epoch: 6/10... Step: 12... Loss: 0.025832... Val Loss: 1.683727\n","Epoch: 6/10... Step: 18... Loss: 0.027226... Val Loss: 1.623015\n","Epoch: 6/10... Step: 24... Loss: 0.070422... Val Loss: 1.727140\n","Epoch: 6/10... Step: 30... Loss: 0.066828... Val Loss: 1.700837\n","Epoch: 7/10... Step: 6... Loss: 0.010650... Val Loss: 1.944731\n","Epoch: 7/10... Step: 12... Loss: 0.004015... Val Loss: 1.996721\n","Epoch: 7/10... Step: 18... Loss: 0.016739... Val Loss: 1.996214\n","Epoch: 7/10... Step: 24... Loss: 0.005271... Val Loss: 2.004686\n","Epoch: 7/10... Step: 30... Loss: 0.054448... Val Loss: 1.710049\n","Epoch: 8/10... Step: 6... Loss: 0.075575... Val Loss: 1.890235\n","Epoch: 8/10... Step: 12... Loss: 0.082579... Val Loss: 1.712474\n","Epoch: 8/10... Step: 18... Loss: 0.024541... Val Loss: 1.927598\n","Epoch: 8/10... Step: 24... Loss: 0.039083... Val Loss: 1.982187\n","Epoch: 8/10... Step: 30... Loss: 0.077715... Val Loss: 1.855470\n","Epoch: 9/10... Step: 6... Loss: 0.054382... Val Loss: 1.600864\n","Epoch: 9/10... Step: 12... Loss: 0.008424... Val Loss: 1.854980\n","Epoch: 9/10... Step: 18... Loss: 0.010126... Val Loss: 2.066600\n","Epoch: 9/10... Step: 24... Loss: 0.005377... Val Loss: 2.159621\n","Epoch: 9/10... Step: 30... Loss: 0.007128... Val Loss: 2.258568\n","Epoch: 10/10... Step: 6... Loss: 0.008295... Val Loss: 2.098264\n","Epoch: 10/10... Step: 12... Loss: 0.001696... Val Loss: 2.155512\n","Epoch: 10/10... Step: 18... Loss: 0.027722... Val Loss: 2.239611\n","Epoch: 10/10... Step: 24... Loss: 0.001172... Val Loss: 2.327893\n","Epoch: 10/10... Step: 30... Loss: 0.001690... Val Loss: 2.410175\n"]}],"source":["# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","val_loss_min = np.Inf\n","\n","# training params\n","\n","epochs = 10 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","print_every = len(train_loader)/6\n","clip=5 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","for e in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    # batch loop\n","    counter = 0\n","    for inputs, labels in train_loader:\n","        counter += 1\n","        if counter == len(train_loader): # the last bacth doesn't have the batch size specified and it gives an error, so we skip it.\n","          continue\n","\n","        if(train_on_gpu):\n","            inputs=inputs.cuda()\n","            labels=labels.cuda()\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        output, h = net(inputs, h)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.float())\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","        # loss stats\n","        if counter % print_every == 0:\n","            # Get validation loss\n","            counter2 = 0\n","            val_h = net.init_hidden(batch_size)\n","            val_losses = []\n","            net.eval()\n","            for inputs, labels in valid_loader:\n","\n","                counter2 += 1\n","                if counter2 == len(valid_loader):\n","                  continue\n","\n","                # Creating new variables for the hidden state, otherwise\n","                # we'd backprop through the entire training history\n","                val_h = tuple([each.data for each in val_h])\n","\n","                if(train_on_gpu):\n","                  inputs, labels = inputs.cuda(), labels.cuda()  \n","\n","                output, val_h = net(inputs, val_h)\n","                val_loss = criterion(output.squeeze(), labels.float())\n","\n","                val_losses.append(val_loss.item())\n","\n","            # create checkpoint variable and add important data\n","            checkpoint = {\n","                  'epoch': e + 1,\n","                  'valid_loss_min': val_loss,\n","                  'state_dict': net.state_dict(),\n","                  'optimizer': optimizer.state_dict()\n","            }\n","\n","            ## TODO: save the model if validation loss has decreased\n","            if val_loss <= val_loss_min:\n","              print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min,val_loss))\n","              # save checkpoint as best model\n","              save_checkpoint(destination_folder + '/lstm_model_' + dataset + '.pt', net, optimizer, val_loss_min)\n","              val_loss_min = val_loss\n","\n","            net.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Loss: {:.6f}...\".format(loss.item()),\n","                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"]},{"cell_type":"markdown","source":["# Test model accuracy"],"metadata":{"id":"FUjgCVJzSsNi"}},{"cell_type":"code","execution_count":51,"metadata":{"id":"Om0dwJ7mieA1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663375616588,"user_tz":-60,"elapsed":11,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}},"outputId":"d9b4167f-6a90-45f6-a8e3-2ba6abf8b25b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss: 0.784\n","Test accuracy: 0.374\n"]}],"source":["# Test model accuracy\n","\n","test_losses = [] # track loss\n","num_correct = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.eval()\n","# iterate over test data\n","counter3 = 0\n","for inputs, labels in test_loader:\n","\n","    counter3 += 1\n","    if counter3 == len(test_loader):\n","      continue\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    h = tuple([each.data for each in h])\n","\n","    if(train_on_gpu):\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","\n","    output, h = net(inputs, h)\n","\n","    # calculate loss\n","    test_loss = criterion(output.squeeze(), labels.float())\n","    test_losses.append(test_loss.item())\n","\n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","\n","    # compare predictions to true label\n","    correct_tensor = pred.eq(labels.float().view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct += np.sum(correct)\n","\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","test_acc = num_correct/len(test_loader.dataset)\n","print(\"Test accuracy: {:.3f}\".format(test_acc))"]}],"metadata":{"colab":{"collapsed_sections":["Se4vnWpASj_k","76ifE2bKSh8N","PCI-NKijSdn8","RIuvY1rLSZOe"],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"989a8ee0261a415be34d4cf0f45e98134ff6fbcaa2e29b3efcaef888d322ba01"}},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}