{"cells":[{"cell_type":"code","source":["# choose dataset from 'NPOV', 'WNC', 'CrowS-Pairs', 'Stereo', 'Mixed'\n","dataset = 'Mixed'"],"metadata":{"id":"sApvh54ogO6v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mP0P1_WTBRwZ"},"source":["# Setting up"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2614,"status":"ok","timestamp":1663296504330,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"SI_13tD70-ZQ","outputId":"6175240b-2ed7-4266-b96f-fcc3c264a560"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3693,"status":"ok","timestamp":1663296507977,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"dxvoHuV7BCpA","outputId":"3fd7a84e-0a06-4842-d3c2-d570c73d6c26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzoL0hO1t0j9"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import shutil\n","import sys\n","from transformers import BloomTokenizerFast, BloomForSequenceClassification\n","from transformers import BertTokenizer, BertModel, BertTokenizerFast\n","\n","import nltk\n","from nltk.parse.stanford import StanfordDependencyParser\n","\n","import sys; sys.path.append('.')\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjK_tPCoBQjd"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"markdown","metadata":{"id":"AlX3tF90Mx2-"},"source":["# Pickle functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VOfsYPRNCap"},"outputs":[],"source":["import os\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9a9BYOc0MwkN"},"outputs":[],"source":["def load_pickle(filename):\n","    completeName = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Amplifi Project/Data/Processed Datasets/\",\\\n","                                filename)\n","    with open(completeName, 'rb') as pkl_file:\n","        data = pickle.load(pkl_file)\n","    return data\n","\n","def save_as_pickle(filename, data):\n","    completeName = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Amplifi Project/Data/Processed Datasets/\",\\\n","                                filename)\n","    with open(completeName, 'wb') as output:\n","        pickle.dump(data, output)"]},{"cell_type":"markdown","metadata":{"id":"7lAASlTuBVCu"},"source":["# TAGS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-2XQ5f0t6uz"},"outputs":[],"source":["# Dependency parser: https://spacy.io/api/dependencyparser\n","# https://www.upgrad.com/blog/dependency-parsing-in-nlp/ \n","RELATIONS = ['<PAD>',\n","    'ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', \n","    'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', \n","    'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg',\n","    'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis',\n","    'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct',\n","    'quantmod', 'relcl', 'xcomp', 'attr', 'csubjpass',\n","    '<UNK>'\n","]\n","REL2ID = {x: i for i, x in enumerate(RELATIONS)}\n","\n","# Part-Of-Speech tagging: https://spacy.io/usage/linguistic-features#pos-tagging \n","POS_TAGS = ['<PAD>',\n","    'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'CONJ', 'DET', 'EOL',\n","    'IDS', 'INTJ', 'NAMES', 'NOUN', 'NO_TAG', 'NUM', 'PART',\n","    'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SPACE', 'SYM', 'VERB', 'X',\n","    '<UNK>'\n","]\n","POS2ID = {x: i for i, x in enumerate(POS_TAGS)} "]},{"cell_type":"markdown","metadata":{"id":"EGk80wOeBXja"},"source":["# Featurizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAuv3HAShM3a"},"outputs":[],"source":["class Featurizer:\n","\n","    def __init__(self, tok2id={}, pad_id=0, lexicon_feature_bits=1):\n","        self.tok2id = tok2id\n","        self.id2tok = {x: tok for tok, x in tok2id.items()}\n","        self.pad_id = pad_id\n","\n","        self.pos2id = POS2ID\n","        self.rel2id = REL2ID\n","\n","        self.lexicons = {\n","            'assertives': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/assertives_hooper1975.txt'),\n","            'entailed_arg': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/entailed_arg_berant2012.txt'),\n","            'entailed': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/entailed_berant2012.txt'), \n","            'entailing_arg': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/entailing_arg_berant2012.txt'), \n","            'entailing': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/entailing_berant2012.txt'), \n","            'factives': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/factives_hooper1975.txt'),\n","            'hedges': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/hedges_hyland2005.txt'),\n","            'implicatives': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/implicatives_karttunen1971.txt'),\n","            'negatives': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/negative_liu2005.txt'),\n","            'positives': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/positive_liu2005.txt'),\n","            'npov': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/npov_lexicon.txt'),\n","            'reports': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/report_verbs.txt'),\n","            'strong_subjectives': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/strong_subjectives_riloff2003.txt'),\n","            'weak_subjectives': self.read_lexicon('/content/drive/MyDrive/Colab Notebooks/Amplifi Project/lexicons/weak_subjectives_riloff2003.txt')\n","        }\n","        self.lexicon_feature_bits = lexicon_feature_bits\n","\n","\n","    def get_feature_names(self):\n","\n","        lexicon_feature_names = list(self.lexicons.keys())\n","        context_feature_names = [x + '_context' for x in lexicon_feature_names]\n","        pos_names = list(list(zip(*sorted(self.pos2id.items(), key=lambda x: x[1])))[0])\n","        rel_names = list(list(zip(*sorted(self.rel2id.items(), key=lambda x: x[1])))[0])\n","\n","        return lexicon_feature_names + context_feature_names + pos_names + rel_names        \n","\n","    def read_lexicon(self, fp):\n","        out = set([\n","            l.strip() for l in open(fp, errors='ignore') \n","            if not l.startswith('#') and not l.startswith(';')\n","            and len(l.strip().split()) == 1\n","        ])\n","        return out\n","\n","\n","    def lexicon_features(self, words, bits=2):\n","        assert bits in [1, 2]\n","        if bits == 1:\n","            true = 1\n","            false = 0\n","        else:\n","            true = [1, 0]\n","            false = [0, 1]\n","    \n","        out = []\n","        for word in words:\n","            out.append([\n","                true if word in lexicon else false \n","                for _, lexicon in self.lexicons.items()\n","            ])\n","        out = np.array(out)\n","\n","        if bits == 2:\n","            out = out.reshape(len(words), -1)\n","\n","        return out\n","\n","\n","    def context_features(self, lex_feats, window_size=2):\n","        out = []\n","        nwords = lex_feats.shape[0]\n","        nfeats = lex_feats.shape[1]\n","        for wi in range(lex_feats.shape[0]):\n","            window_start = max(wi - window_size, 0)\n","            window_end = min(wi + window_size + 1, nwords)\n","\n","            left = lex_feats[window_start: wi, :] if wi > 0 else np.zeros((1, nfeats))\n","            right = lex_feats[wi + 1: window_end, :] if wi < nwords - 1 else np.zeros((1, nfeats))\n","\n","            out.append((np.sum(left + right, axis=0) > 0).astype(int))\n","\n","        return np.array(out)\n","\n","\n","    def features(self, id_seq, rel_ids, pos_ids):\n","        if self.pad_id in id_seq:\n","            pad_idx = id_seq.index(self.pad_id)\n","            pad_len = len(id_seq[pad_idx:])\n","            id_seq = id_seq[:pad_idx]\n","            rel_ids = rel_ids[:pad_idx]\n","            pos_ids = pos_ids[:pad_idx]\n","        else:\n","            pad_len = 0\n","\n","        toks = [self.id2tok[x] for x in id_seq]\n","        # build list of [word, [tok indices the word came from]]\n","        words = []\n","        word_indices = []\n","        for i, tok in enumerate(toks):\n","            if tok.startswith('##'):\n","                words[-1] += tok.replace('##', '')\n","                word_indices[-1].append(i)\n","            else:\n","                words.append(tok)\n","                word_indices.append([i])\n","\n","        # get expert features\n","        lex_feats = self.lexicon_features(words, bits=self.lexicon_feature_bits)\n","        context_feats = self.context_features(lex_feats)\n","        expert_feats = np.concatenate((lex_feats, context_feats), axis=1)\n","        # break word-features into tokens\n","        feats = np.concatenate([\n","            np.repeat(np.expand_dims(word_vec, axis=0), len(indices), axis=0) \n","            for (word_vec, indices) in zip(expert_feats, word_indices)\n","        ], axis=0)\n","\n","        # add in the pos and relational features\n","        pos_feats = np.zeros((len(pos_ids), len(POS2ID)))\n","        pos_feats[range(len(pos_ids)), pos_ids] = 1\n","        rel_feats = np.zeros((len(rel_ids), len(REL2ID)))\n","        rel_feats[range(len(rel_ids)), rel_ids] = 1\n","        \n","        feats = np.concatenate((feats, pos_feats, rel_feats), axis=1)\n","\n","        # add pad back in                \n","        feats = np.concatenate((feats, np.zeros((pad_len, feats.shape[1]))))\n","\n","        return feats\n","\n","\n","    def featurize_batch(self, batch_ids, rel_ids, pos_ids, padded_len=0):\n","        \"\"\" takes [batch, len] returns [batch, len, features] --> i.e. for every sentence in the batch, for every word in len we have the features\"\"\"\n","\n","        batch_feats = [\n","            self.features(list(id_seq), list(rel_ids), list(pos_ids)) \n","            for id_seq, rel_ids, pos_ids in zip(batch_ids, rel_ids, pos_ids)]\n","        batch_feats = np.array(batch_feats)\n","        return batch_feats\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UMY_xtSNzaK9"},"source":["# Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgi1udTsylsL"},"outputs":[],"source":["# import datasets\n","source_folder = \"/content/drive/MyDrive/Colab Notebooks/Amplifi Project/Data/Processed Datasets/\"\n","train_df = pd.read_csv(source_folder + dataset + '_train.csv', delimiter=',')\n","val_df = pd.read_csv(source_folder + dataset + '_valid.csv', delimiter=',')\n","test_df = pd.read_csv(source_folder + dataset + '_test.csv', delimiter=',')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNYPze_7zbtm"},"outputs":[],"source":["# hyperparameters\n","max_len = 200 # we cap the max length at 200, even though some data samples have larger length\n","train_batch_size = 32\n","val_batch_size = 32\n","test_batch_size = 32\n","\n","target_list = ['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_fg1XkIXFIt"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","source":["a = tokenizer.vocab\n","print(a['the'], a['chess'], a['player'], a['is'], a['always'], a['a'], a['geek'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-O7PmdM2kJo","executionInfo":{"status":"ok","timestamp":1663296511799,"user_tz":-60,"elapsed":15,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}},"outputId":"ad0466ec-6653-46bf-e8ce-d0da293481d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1996 7433 2447 2003 2467 1037 29294\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdoY9lTmzuVR"},"outputs":[],"source":["class CustomDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.df = df\n","        self.title = df['text']\n","        self.pos = df['pos_tags']\n","        self.rel = df['rel_tags']\n","        self.targets = self.df[target_list].values\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.title)\n","\n","    def __getitem__(self, index):\n","        title = str(self.title[index])\n","        title = \" \".join(title.split())\n","\n","        pos_str = str(self.pos[index])\n","        pos = []\n","        for tag in pos_str.split():\n","          try:\n","            pos.append(POS2ID[tag])\n","          except:\n","            POS2ID['<UNK>']\n","        pos = torch.tensor(pos)\n","        pos_ids = torch.zeros(max_len, dtype=int)\n","        if len(pos) > max_len:\n","          pos_ids[:] = pos[:max_len]\n","        else:\n","          pos_ids[:len(pos)] = pos\n","\n","        rel_str = str(self.rel[index])\n","        rel = []\n","        for tag in rel_str.split():\n","          try:\n","            rel.append(REL2ID[tag])\n","          except:\n","            REL2ID['<UNK>']\n","        rel = torch.tensor(rel)\n","        rel_ids = torch.zeros(max_len, dtype=int)\n","        if len(rel) > max_len:\n","          rel_ids[:] = rel[:max_len]\n","        else:\n","          rel_ids[:len(rel)] = rel\n","\n","        inputs = self.tokenizer.encode_plus(\n","            title,\n","            None,\n","            add_special_tokens=False, # we do not want SOS and EOS tokens\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=False, # we cannot have this if we do not have SOS and EOS tokens\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'text': title,\n","            'input_ids': inputs['input_ids'].flatten(), # ID of each token in the text\n","            'pos_ids': pos_ids,\n","            'rel_ids': rel_ids,\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            # 'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n","            'targets': torch.FloatTensor(self.targets[index]) # Bias and Unbias label (0 when False, 1 when True)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMBczZ-F0G5M"},"outputs":[],"source":["train_dataset = CustomDataset(train_df, tokenizer, max_len)\n","val_dataset = CustomDataset(val_df, tokenizer, max_len)\n","test_dataset = CustomDataset(test_df, tokenizer, max_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRdmTNW30IjD"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(train_dataset, \n","    batch_size=train_batch_size,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = torch.utils.data.DataLoader(val_dataset, \n","    batch_size=val_batch_size,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","test_loader = torch.utils.data.DataLoader(test_dataset, \n","    batch_size=test_batch_size,\n","    shuffle=True,\n","    num_workers=0\n",")"]},{"cell_type":"markdown","metadata":{"id":"VLAyQr3jBwMK"},"source":["# Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H65-o5vjDnNW"},"outputs":[],"source":["tok2id = tokenizer.vocab\n","tok2id['<del>'] = len(tok2id)\n","lexicon_feature_bits = 1\n","featurizer = Featurizer(tok2id, lexicon_feature_bits=lexicon_feature_bits)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apNgGMcHhVma"},"outputs":[],"source":["for data in train_loader:\n","  break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWeF1wLnZE6B"},"outputs":[],"source":["feats = featurizer.featurize_batch(\n","            data['input_ids'].detach().cpu().numpy(), \n","            data['rel_ids'].detach().cpu().numpy(), \n","            data['pos_ids'].detach().cpu().numpy(), \n","            padded_len=data['input_ids'].shape[1])"]},{"cell_type":"code","source":["feats.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ussbSf_Bm_9S","executionInfo":{"status":"ok","timestamp":1663296512281,"user_tz":-60,"elapsed":12,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}},"outputId":"4bc48984-705e-4f90-a016-4ee1c1ccb844"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(32, 200, 100)"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uw62fAgTAaYq"},"outputs":[],"source":["feats = feats.reshape(32,-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1663296512282,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"rzwXOHZ936Tm","outputId":"a9be43b9-ef26-4ecd-9029-f941b53b7015"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["104.0"]},"metadata":{},"execution_count":85}],"source":["sum(feats[11])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1663296512586,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"YVbMvY8FzUJN","outputId":"1b654ba8-a760-4cee-8c6f-941234ed3dd0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(32, 20000)"]},"metadata":{},"execution_count":86}],"source":["feats.shape"]},{"cell_type":"markdown","metadata":{"id":"W_diGAld7E0G"},"source":["# Simple classifier model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"holh8sjP7Gft"},"outputs":[],"source":["# number of features (len of X cols)\n","input_dim = 20000\n","# number of hidden layers\n","hidden_layer1 = 10000\n","hidden_layer2 = 1000\n","# number of classes (unique of y)\n","output_dim = 1\n","\n","class Classifier(nn.Module):\n","  def __init__(self):\n","    super(Classifier, self).__init__()\n","    self.linear1 = nn.Linear(input_dim, hidden_layer1)\n","    self.linear2 = nn.Linear(hidden_layer1, hidden_layer2)\n","    self.linear3 = nn.Linear(hidden_layer2, output_dim)\n","    self.dropout = torch.nn.Dropout(0.3)\n","  def forward(self, x):\n","    x = self.dropout(torch.sigmoid(self.linear1(x)))\n","    x = self.dropout(torch.sigmoid(self.linear2(x)))\n","    x = torch.sigmoid(self.linear3(x))\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"Qpg0Fayj8x33"},"source":["# Prep for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wC5boBzC9Vtn"},"outputs":[],"source":["# Save and Load Functions\n","\n","def save_checkpoint(save_path, model, optimizer, valid_loss):\n","\n","    if save_path == None:\n","        return\n","    \n","    state_dict = {'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict(),\n","                  'valid_loss': valid_loss}\n","    \n","    torch.save(state_dict, save_path)\n","    print(f'Model saved to ==> {save_path}')\n","\n","\n","def load_checkpoint(load_path, model, optimizer):\n","\n","    if load_path==None:\n","        return\n","    \n","    state_dict = torch.load(load_path, map_location=device)\n","    print(f'Model loaded from <== {load_path}')\n","    \n","    model.load_state_dict(state_dict['model_state_dict'])\n","    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n","    \n","    return state_dict['valid_loss']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6VzZu9-9CGr"},"outputs":[],"source":["def loss_fn(outputs, targets):\n","    return torch.nn.BCELoss()(outputs, targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27463,"status":"ok","timestamp":1663296548945,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"},"user_tz":-60},"id":"zBUF69l18zcZ","outputId":"3ae5ae92-a63f-4c72-9639-0bd628f87d80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded from <== /content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints/features_model_Mixed.pt\n","features_model_Mixed.pt --> loaded\n"]}],"source":["model = Classifier()\n","model.to(device)\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-5)\n","\n","destination_folder = \"/content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints\"\n","\n","try:\n","  load_checkpoint(destination_folder + '/features_model_' + dataset + '.pt', model, optimizer) # comment this if you wannt to train the model from zero\n","  print('features_model_' + dataset + '.pt --> loaded')\n","except:\n","  print('training features_model_' + dataset + ' from scratch')"]},{"cell_type":"markdown","metadata":{"id":"yZlEe6OmL4mR"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0K21YVnL5z1"},"outputs":[],"source":["val_targets=[]\n","val_outputs=[]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFGdne4IL8jj"},"outputs":[],"source":["def train_model(n_epochs, featurizer, training_loader, validation_loader, model, \n","                optimizer, destination_folder):\n","   \n","  # initialize tracker for minimum validation loss\n","  best_valid_loss = np.Inf\n","  # best valid loss epoch 10\n","  # best_valid_loss = 0.039393\n","   \n","  for epoch in range(1, n_epochs+1):\n","    train_loss = 0\n","    valid_loss = 0\n","\n","    model.train()\n","    print('############# Epoch {}: Training Start   #############'.format(epoch))\n","    for batch_idx, data in enumerate(training_loader):\n","\n","        if batch_idx % round(len(training_loader)/6) == 0:\n","          print(f'--> epoch: {epoch}/{n_epochs} --- step {batch_idx}/{len(training_loader)}')\n","\n","        #print('yyy epoch', batch_idx)\n","        # ids = data['input_ids'].to(device, dtype = torch.long)\n","        # mask = data['attention_mask'].to(device, dtype = torch.long)\n","        # token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float)\n","\n","        feats = featurizer.featurize_batch(\n","                data['input_ids'].detach().cpu().numpy(), \n","                data['rel_ids'].detach().cpu().numpy(), \n","                data['pos_ids'].detach().cpu().numpy(), \n","                padded_len=data['input_ids'].shape[1])\n","        \n","        feats = torch.tensor(feats.reshape(feats.shape[0],-1)).to(device, dtype = torch.float)\n","\n","        outputs = model(feats)\n","\n","        optimizer.zero_grad()\n","        loss = loss_fn(outputs, targets)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n","        train_loss += loss.item()\n","    \n","    print('############# Epoch {}: Training End     #############'.format(epoch))\n","\n","    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n","    ######################    \n","    # validate the model #\n","    ######################\n"," \n","    model.eval()\n","   \n","    with torch.no_grad():\n","      for batch_idx, data in enumerate(validation_loader, 0):\n","\n","            if batch_idx % round(len(validation_loader)/6) == 0:\n","              print(f'--> epoch: {epoch}/{n_epochs} --- step {batch_idx}/{len(validation_loader)}')\n","            \n","            # ids = data['input_ids'].to(device, dtype = torch.long)\n","            # mask = data['attention_mask'].to(device, dtype = torch.long)\n","            # token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","\n","            feats = featurizer.featurize_batch(\n","                    data['input_ids'].detach().cpu().numpy(), \n","                    data['rel_ids'].detach().cpu().numpy(), \n","                    data['pos_ids'].detach().cpu().numpy(), \n","                    padded_len=data['input_ids'].shape[1])\n","                \n","            feats = torch.tensor(feats.reshape(feats.shape[0],-1)).to(device, dtype = torch.float)\n","\n","            outputs = model(feats)\n","\n","            loss = loss_fn(outputs, targets)\n","            # valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n","            valid_loss += loss.item()\n","            val_targets.extend(targets.cpu().detach().numpy().tolist())\n","            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","      print('############# Epoch {}: Validation End     #############'.format(epoch))\n","      # calculate average losses\n","      #print('before cal avg train loss', train_loss)\n","      train_loss = train_loss/len(training_loader)\n","      valid_loss = valid_loss/len(validation_loader)\n","      # print training/validation statistics \n","      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n","            epoch, \n","            train_loss,\n","            valid_loss\n","            ))\n","        \n","      ## TODO: save the model if validation loss has decreased\n","      if valid_loss <= best_valid_loss:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_valid_loss,valid_loss))\n","        # save checkpoint as best model\n","        save_checkpoint(destination_folder + '/features_model_' + dataset + '.pt', model, optimizer, best_valid_loss)\n","        best_valid_loss = valid_loss\n","\n","    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpO3RgaWMPQd"},"outputs":[],"source":["tok2id = tokenizer.vocab\n","tok2id['<del>'] = len(tok2id)\n","lexicon_feature_bits = 1\n","featurizer = Featurizer(tok2id, lexicon_feature_bits=lexicon_feature_bits)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPO4YkkCMP1z","outputId":"83d899f2-4683-449d-d75c-ff42abdee4c3","executionInfo":{"status":"ok","timestamp":1663295663105,"user_tz":-60,"elapsed":94944,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["############# Epoch 1: Training Start   #############\n","--> epoch: 1/10 --- step 0/56\n","--> epoch: 1/10 --- step 9/56\n","--> epoch: 1/10 --- step 18/56\n","--> epoch: 1/10 --- step 27/56\n","--> epoch: 1/10 --- step 36/56\n","--> epoch: 1/10 --- step 45/56\n","--> epoch: 1/10 --- step 54/56\n","############# Epoch 1: Training End     #############\n","############# Epoch 1: Validation Start   #############\n","--> epoch: 1/10 --- step 0/7\n","--> epoch: 1/10 --- step 1/7\n","--> epoch: 1/10 --- step 2/7\n","--> epoch: 1/10 --- step 3/7\n","--> epoch: 1/10 --- step 4/7\n","--> epoch: 1/10 --- step 5/7\n","--> epoch: 1/10 --- step 6/7\n","############# Epoch 1: Validation End     #############\n","Epoch: 1 \tAvgerage Training Loss: 0.708098 \tAverage Validation Loss: 0.693474\n","Validation loss decreased (inf --> 0.693474).  Saving model ...\n","Model saved to ==> /content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints/features_model_Stereo.pt\n","############# Epoch 1  Done   #############\n","\n","############# Epoch 2: Training Start   #############\n","--> epoch: 2/10 --- step 0/56\n","--> epoch: 2/10 --- step 9/56\n","--> epoch: 2/10 --- step 18/56\n","--> epoch: 2/10 --- step 27/56\n","--> epoch: 2/10 --- step 36/56\n","--> epoch: 2/10 --- step 45/56\n","--> epoch: 2/10 --- step 54/56\n","############# Epoch 2: Training End     #############\n","############# Epoch 2: Validation Start   #############\n","--> epoch: 2/10 --- step 0/7\n","--> epoch: 2/10 --- step 1/7\n","--> epoch: 2/10 --- step 2/7\n","--> epoch: 2/10 --- step 3/7\n","--> epoch: 2/10 --- step 4/7\n","--> epoch: 2/10 --- step 5/7\n","--> epoch: 2/10 --- step 6/7\n","############# Epoch 2: Validation End     #############\n","Epoch: 2 \tAvgerage Training Loss: 0.692987 \tAverage Validation Loss: 0.692415\n","Validation loss decreased (0.693474 --> 0.692415).  Saving model ...\n","Model saved to ==> /content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints/features_model_Stereo.pt\n","############# Epoch 2  Done   #############\n","\n","############# Epoch 3: Training Start   #############\n","--> epoch: 3/10 --- step 0/56\n","--> epoch: 3/10 --- step 9/56\n","--> epoch: 3/10 --- step 18/56\n","--> epoch: 3/10 --- step 27/56\n","--> epoch: 3/10 --- step 36/56\n","--> epoch: 3/10 --- step 45/56\n","--> epoch: 3/10 --- step 54/56\n","############# Epoch 3: Training End     #############\n","############# Epoch 3: Validation Start   #############\n","--> epoch: 3/10 --- step 0/7\n","--> epoch: 3/10 --- step 1/7\n","--> epoch: 3/10 --- step 2/7\n","--> epoch: 3/10 --- step 3/7\n","--> epoch: 3/10 --- step 4/7\n","--> epoch: 3/10 --- step 5/7\n","--> epoch: 3/10 --- step 6/7\n","############# Epoch 3: Validation End     #############\n","Epoch: 3 \tAvgerage Training Loss: 0.694799 \tAverage Validation Loss: 0.694874\n","############# Epoch 3  Done   #############\n","\n","############# Epoch 4: Training Start   #############\n","--> epoch: 4/10 --- step 0/56\n","--> epoch: 4/10 --- step 9/56\n","--> epoch: 4/10 --- step 18/56\n","--> epoch: 4/10 --- step 27/56\n","--> epoch: 4/10 --- step 36/56\n","--> epoch: 4/10 --- step 45/56\n","--> epoch: 4/10 --- step 54/56\n","############# Epoch 4: Training End     #############\n","############# Epoch 4: Validation Start   #############\n","--> epoch: 4/10 --- step 0/7\n","--> epoch: 4/10 --- step 1/7\n","--> epoch: 4/10 --- step 2/7\n","--> epoch: 4/10 --- step 3/7\n","--> epoch: 4/10 --- step 4/7\n","--> epoch: 4/10 --- step 5/7\n","--> epoch: 4/10 --- step 6/7\n","############# Epoch 4: Validation End     #############\n","Epoch: 4 \tAvgerage Training Loss: 0.698810 \tAverage Validation Loss: 0.691665\n","Validation loss decreased (0.692415 --> 0.691665).  Saving model ...\n","Model saved to ==> /content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints/features_model_Stereo.pt\n","############# Epoch 4  Done   #############\n","\n","############# Epoch 5: Training Start   #############\n","--> epoch: 5/10 --- step 0/56\n","--> epoch: 5/10 --- step 9/56\n","--> epoch: 5/10 --- step 18/56\n","--> epoch: 5/10 --- step 27/56\n","--> epoch: 5/10 --- step 36/56\n","--> epoch: 5/10 --- step 45/56\n","--> epoch: 5/10 --- step 54/56\n","############# Epoch 5: Training End     #############\n","############# Epoch 5: Validation Start   #############\n","--> epoch: 5/10 --- step 0/7\n","--> epoch: 5/10 --- step 1/7\n","--> epoch: 5/10 --- step 2/7\n","--> epoch: 5/10 --- step 3/7\n","--> epoch: 5/10 --- step 4/7\n","--> epoch: 5/10 --- step 5/7\n","--> epoch: 5/10 --- step 6/7\n","############# Epoch 5: Validation End     #############\n","Epoch: 5 \tAvgerage Training Loss: 0.693435 \tAverage Validation Loss: 0.713420\n","############# Epoch 5  Done   #############\n","\n","############# Epoch 6: Training Start   #############\n","--> epoch: 6/10 --- step 0/56\n","--> epoch: 6/10 --- step 9/56\n","--> epoch: 6/10 --- step 18/56\n","--> epoch: 6/10 --- step 27/56\n","--> epoch: 6/10 --- step 36/56\n","--> epoch: 6/10 --- step 45/56\n","--> epoch: 6/10 --- step 54/56\n","############# Epoch 6: Training End     #############\n","############# Epoch 6: Validation Start   #############\n","--> epoch: 6/10 --- step 0/7\n","--> epoch: 6/10 --- step 1/7\n","--> epoch: 6/10 --- step 2/7\n","--> epoch: 6/10 --- step 3/7\n","--> epoch: 6/10 --- step 4/7\n","--> epoch: 6/10 --- step 5/7\n","--> epoch: 6/10 --- step 6/7\n","############# Epoch 6: Validation End     #############\n","Epoch: 6 \tAvgerage Training Loss: 0.693694 \tAverage Validation Loss: 0.692078\n","############# Epoch 6  Done   #############\n","\n","############# Epoch 7: Training Start   #############\n","--> epoch: 7/10 --- step 0/56\n","--> epoch: 7/10 --- step 9/56\n","--> epoch: 7/10 --- step 18/56\n","--> epoch: 7/10 --- step 27/56\n","--> epoch: 7/10 --- step 36/56\n","--> epoch: 7/10 --- step 45/56\n","--> epoch: 7/10 --- step 54/56\n","############# Epoch 7: Training End     #############\n","############# Epoch 7: Validation Start   #############\n","--> epoch: 7/10 --- step 0/7\n","--> epoch: 7/10 --- step 1/7\n","--> epoch: 7/10 --- step 2/7\n","--> epoch: 7/10 --- step 3/7\n","--> epoch: 7/10 --- step 4/7\n","--> epoch: 7/10 --- step 5/7\n","--> epoch: 7/10 --- step 6/7\n","############# Epoch 7: Validation End     #############\n","Epoch: 7 \tAvgerage Training Loss: 0.697138 \tAverage Validation Loss: 0.726603\n","############# Epoch 7  Done   #############\n","\n","############# Epoch 8: Training Start   #############\n","--> epoch: 8/10 --- step 0/56\n","--> epoch: 8/10 --- step 9/56\n","--> epoch: 8/10 --- step 18/56\n","--> epoch: 8/10 --- step 27/56\n","--> epoch: 8/10 --- step 36/56\n","--> epoch: 8/10 --- step 45/56\n","--> epoch: 8/10 --- step 54/56\n","############# Epoch 8: Training End     #############\n","############# Epoch 8: Validation Start   #############\n","--> epoch: 8/10 --- step 0/7\n","--> epoch: 8/10 --- step 1/7\n","--> epoch: 8/10 --- step 2/7\n","--> epoch: 8/10 --- step 3/7\n","--> epoch: 8/10 --- step 4/7\n","--> epoch: 8/10 --- step 5/7\n","--> epoch: 8/10 --- step 6/7\n","############# Epoch 8: Validation End     #############\n","Epoch: 8 \tAvgerage Training Loss: 0.695199 \tAverage Validation Loss: 0.694638\n","############# Epoch 8  Done   #############\n","\n","############# Epoch 9: Training Start   #############\n","--> epoch: 9/10 --- step 0/56\n","--> epoch: 9/10 --- step 9/56\n","--> epoch: 9/10 --- step 18/56\n","--> epoch: 9/10 --- step 27/56\n","--> epoch: 9/10 --- step 36/56\n","--> epoch: 9/10 --- step 45/56\n","--> epoch: 9/10 --- step 54/56\n","############# Epoch 9: Training End     #############\n","############# Epoch 9: Validation Start   #############\n","--> epoch: 9/10 --- step 0/7\n","--> epoch: 9/10 --- step 1/7\n","--> epoch: 9/10 --- step 2/7\n","--> epoch: 9/10 --- step 3/7\n","--> epoch: 9/10 --- step 4/7\n","--> epoch: 9/10 --- step 5/7\n","--> epoch: 9/10 --- step 6/7\n","############# Epoch 9: Validation End     #############\n","Epoch: 9 \tAvgerage Training Loss: 0.692033 \tAverage Validation Loss: 0.692022\n","############# Epoch 9  Done   #############\n","\n","############# Epoch 10: Training Start   #############\n","--> epoch: 10/10 --- step 0/56\n","--> epoch: 10/10 --- step 9/56\n","--> epoch: 10/10 --- step 18/56\n","--> epoch: 10/10 --- step 27/56\n","--> epoch: 10/10 --- step 36/56\n","--> epoch: 10/10 --- step 45/56\n","--> epoch: 10/10 --- step 54/56\n","############# Epoch 10: Training End     #############\n","############# Epoch 10: Validation Start   #############\n","--> epoch: 10/10 --- step 0/7\n","--> epoch: 10/10 --- step 1/7\n","--> epoch: 10/10 --- step 2/7\n","--> epoch: 10/10 --- step 3/7\n","--> epoch: 10/10 --- step 4/7\n","--> epoch: 10/10 --- step 5/7\n","--> epoch: 10/10 --- step 6/7\n","############# Epoch 10: Validation End     #############\n","Epoch: 10 \tAvgerage Training Loss: 0.695654 \tAverage Validation Loss: 0.706460\n","############# Epoch 10  Done   #############\n","\n"]}],"source":["epochs = 10\n","trained_model = train_model(epochs, featurizer, train_loader, val_loader, model, optimizer, destination_folder)"]},{"cell_type":"markdown","metadata":{"id":"LiiTCglsNohu"},"source":["# Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6TFanQFENpd5","outputId":"5a5c70c5-8238-49c2-a42d-8e9b66b51760","executionInfo":{"status":"ok","timestamp":1663296575719,"user_tz":-60,"elapsed":17715,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded from <== /content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints/features_model_Mixed.pt\n","accuracy = 59.9 %\n"]}],"source":["# testing\n","corrects = 0\n","totals = 0\n","\n","test_model = Classifier()\n","test_model.to(device)\n","optimizer = torch.optim.Adam(params =  test_model.parameters(), lr=1e-5)\n","destination_folder = \"/content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints\"\n","load_checkpoint(destination_folder + '/features_model_' + dataset + '.pt', test_model, optimizer)\n","\n","with torch.no_grad():\n","  test_model.eval() \n","  for batch_idx, data in enumerate(test_loader, 0): \n","    feats = featurizer.featurize_batch(\n","                data['input_ids'].detach().cpu().numpy(), \n","                data['rel_ids'].detach().cpu().numpy(), \n","                data['pos_ids'].detach().cpu().numpy(), \n","                padded_len=data['input_ids'].shape[1])\n","        \n","    feats = torch.tensor(feats.reshape(feats.shape[0],-1)).to(device, dtype = torch.float)\n","\n","    outputs = test_model(feats)\n","    final_output = outputs.cpu().detach().numpy().tolist()\n","    pred_class = np.round(final_output)\n","    corrects += np.count_nonzero((data['targets'] - pred_class) == 0.)\n","    totals += len(pred_class)\n","  \n","  # Calculate accuracy of test set\n","  acc = corrects / totals\n","\n","  print(f'accuracy = {acc * 100} %')"]},{"cell_type":"code","source":["# Evaluation Function\n","# Evaluation\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import seaborn as sns\n","\n","def evaluate(model, test_loader, version='title'):\n","    y_pred = []\n","    y_true = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(test_loader, 0): \n","            feats = featurizer.featurize_batch(\n","                        data['input_ids'].detach().cpu().numpy(), \n","                        data['rel_ids'].detach().cpu().numpy(), \n","                        data['pos_ids'].detach().cpu().numpy(), \n","                        padded_len=data['input_ids'].shape[1])\n","        \n","            feats = torch.tensor(feats.reshape(feats.shape[0],-1)).to(device, dtype = torch.float)\n","\n","            output = model(feats)\n","            output = output.cpu().detach().numpy().tolist()\n","            output = np.round(output)\n","            y_pred.extend(output.tolist())\n","            y_true.extend(data['targets'].tolist())\n","    \n","    print('Classification Report:')\n","    print('remember: 1 = BIASED, 0 = UNBIASED')\n","    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n","    \n","    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n","    ax= plt.subplot()\n","    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n","\n","    ax.set_title('Confusion Matrix')\n","\n","    ax.set_xlabel('Predicted Labels')\n","    ax.set_ylabel('True Labels')\n","\n","    ax.xaxis.set_ticklabels(['BIASED', 'UNBIASED'])\n","    ax.yaxis.set_ticklabels(['BIASED', 'UNBIASED'])\n","    \n","    \n","best_model = Classifier().to(device)\n","optimizer = torch.optim.Adam(params =  best_model.parameters(), lr=1e-5)\n","\n","load_checkpoint(destination_folder + '/features_model_' + dataset + '.pt', best_model, optimizer)\n","evaluate(best_model, test_loader)"],"metadata":{"id":"nBAc5XEydYxE","colab":{"base_uri":"https://localhost:8080/","height":509},"executionInfo":{"status":"ok","timestamp":1663296583349,"user_tz":-60,"elapsed":7649,"user":{"displayName":"Jo Rossello","userId":"16140766760579671400"}},"outputId":"142daa93-301d-4a1a-fe55-e6b5fb985251"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded from <== /content/drive/MyDrive/Colab Notebooks/Amplifi Project/Saved Models and Checkpoints/features_model_Mixed.pt\n","Classification Report:\n","remember: 1 = BIASED, 0 = UNBIASED\n","              precision    recall  f1-score   support\n","\n","           1     0.5970    0.6914    0.6408       525\n","           0     0.5867    0.4842    0.5306       475\n","\n","    accuracy                         0.5930      1000\n","   macro avg     0.5919    0.5878    0.5857      1000\n","weighted avg     0.5921    0.5930    0.5884      1000\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW4AAAEWCAYAAABG030jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5xdVbn/8c930oFAqhBCQgiEJiUUAWlCKCKCFBVEBES4EQSvUoQf6qUE1ItKkUvRKCBdSgCpQqRIhwQIkBBKQuihpJAQCCEJz++PvSacDDNnzkzOnjM7+b597ZfnrL33WutkDs+sefbaaysiMDOz4qirdQfMzKxlHLjNzArGgdvMrGAcuM3MCsaB28ysYBy4zcwKxoHblpikbpJulTRL0vVLUM+Bku6uZt9qQdKdkg6pdT9s6eXAvQyR9H1JYyXNkTQ1BZhtq1D1d4CVgd4R8d3WVhIRV0XErlXoz2Ik7SApJN3UoHzjVH5/hfWcKunK5o6LiG9ExGWt7K5Zsxy4lxGSjgXOBX5LFmQHAhcCe1Wh+tWBlyJiQRXqysv7wFcl9S4pOwR4qVoNKOP/pix3/pItAyStBIwAjoqIGyPio4iYHxG3RsQv0jFdJJ0r6e20nSupS9q3g6Q3JR0n6b00Wj807TsNOBnYP43kD2s4MpU0KI1sO6b3P5T0iqQPJU2RdGBJ+UMl520taUxKwYyRtHXJvvslnS7p4VTP3ZL6lPln+BS4GfheOr8DsD9wVYN/qz9JekPSbElPStoule8G/LLkcz5T0o/fSHoY+BgYnMoOT/svkjSqpP4zJd0jSRX/AM0acOBeNnwV6ArcVOaYXwFbAUOBjYEtgF+X7F8FWAnoDxwGXCCpZ0ScQjaKvzYiVoiIi8t1RNLywHnANyKiO7A1MK6R43oBt6djewNnA7c3GDF/HzgU+BLQGTi+XNvA5cDB6fXXgfHA2w2OGUP2b9ALuBq4XlLXiPhXg8+5cck5BwHDge7Aaw3qOw7YMP1S2o7s3+6Q8FoTtgQcuJcNvYFpzaQyDgRGRMR7EfE+cBpZQKo3P+2fHxF3AHOAdVrZn8+ADSR1i4ipETGhkWO+CbwcEVdExIKIuAZ4Adiz5JhLI+KliJgLXEcWcJsUEY8AvSStQxbAL2/kmCsjYnpq8yygC81/zr9HxIR0zvwG9X1M9u94NnAl8NOIeLOZ+szKcuBeNkwH+tSnKpqwKouPFl9LZYvqaBD4PwZWaGlHIuIjshTFEcBUSbdLWreC/tT3qX/J+3da0Z8rgKOBHWnkLxBJx0uamNIzH5D9lVEuBQPwRrmdEfE48Aogsl8wZkvEgXvZ8CgwD9i7zDFvk11krDeQL6YRKvURsFzJ+1VKd0bEXRGxC9CPbBT91wr6U9+nt1rZp3pXAD8B7kij4UVSKuMEYD+gZ0T0AGaRBVyAptIbZdMeko4iG7m/neo3WyIO3MuAiJhFdgHxAkl7S1pOUidJ35D0+3TYNcCvJfVNF/lOJvvTvjXGAdtLGpgujJ5Uv0PSypL2SrnueWQpl88aqeMOYO00hbGjpP2B9YHbWtknACJiCvA1spx+Q92BBWQzUDpKOhlYsWT/u8CglswckbQ2cAbwA7KUyQmSyqZ0zJrjwL2MSPnaY8kuOL5P9uf90WQzLSALLmOBZ4HngKdSWWvaGg1cm+p6ksWDbV3qx9vADLIgemQjdUwH9iC7uDedbKS6R0RMa02fGtT9UEQ09tfEXcC/yKYIvgZ8wuJpkPqbi6ZLeqq5dlJq6krgzIh4JiJeJpuZckX9jB2z1pAvbpuZFYtH3GZmBePAbWZWMA7cZmYF48BtZlYw5W7IqKlumxztq6b2BTPHnF/rLlg71LUjS7z2S0tiztynz6/pWjMecZuZFUy7HXGbmbWpAq3I68BtZgZQ16HWPaiYA7eZGUCBlkh34DYzA6dKzMwKxyNuM7OC8YjbzKxgPOI2MysYzyoxMyuYAqVKitNTM7M8SZVvZatRV0lPSHpG0gRJp6Xyv0uaImlc2oamckk6T9IkSc9K2rS5rnrEbWYG1RxxzwOGRcQcSZ2AhyTdmfb9IiJuaHD8N4AhadsSuCj9f5McuM3MoGqBO7LHis1JbzulrdwCVnsBl6fzHpPUQ1K/iJja1AlOlZiZAXToUPEmabiksSXb8NKqJHWQNA54DxgdEY+nXb9J6ZBzSp472p/Fn236ZiprkkfcZmbQoumAETESGFlm/0JgqKQewE2SNgBOAt4BOqdzTwRGtKarHnGbmUGWKql0q1BEfADcB+wWEVMjMw+4FNgiHfYWMKDktNVSWZMcuM3MoJqzSvqmkTaSugG7AC9I6pfKBOwNjE+n3AIcnGaXbAXMKpffBqdKzMwy1ZtV0g+4TFIHssHxdRFxm6R7JfUFBIwDjkjH3wHsDkwCPgYOba4BB24zM6jaLe8R8SywSSPlw5o4PoCjWtKGA7eZGfiWdzOzwinQLe8O3GZm4NUBzcwKxyNuM7OCceA2MysYX5w0MysY57jNzArGqRIzs4LxiNvMrFjkwG1mViwO3GZmBaM6B24zs0LxiNvMrGAcuM3MCsaB28ysaIoTtx24zczAI24zs8KpqyvOnZPF6amZWY4kVbw1U09XSU9IekbSBEmnpfKrJL0oabykSyR1SuU7SJolaVzaTm6ur7mOuNOTjoekty9FxKw82zMza7XqZUrmAcMiYk4Kzg9JuhO4CvhBOuZq4HDgovT+wYjYo9IGcgnckroAfyF7BP0Usn+S1SXdBBwREZ/m0a6ZWWtVK8edHv47J73tlLaIiDtK2noCWK21beSVKvkVWWcHRMQmETEUGEj2i+J/cmrTzKzVWpIqkTRc0tiSbXiDujpIGge8B4yOiMdL9nUCDgL+VXLKV1Nq5U5JX26ur3mlSvYFtoiIj+sLIuJDST8BHsPB28zamZbc8h4RI4GRZfYvBIamdPFNkjaIiPFp94XAAxHxYHr/FLB6Sq3sDtzM5ynmRuU14v6sNGjXi4g5QOTUpplZq1Xr4mSpiPgAuA/YLbVxCtAXOLbkmNkpNpLSKZ0k9SlXb14j7pDUk8bT/Z/l1KaZWatVK8ctqS8wPyI+kNQN2AU4U9LhwNeBnSLis5LjVwHejYiQtAXZgHp6uTbyCtwrAU/SeOD2iNvM2p0q3oDTD7hMUgeyIHxdRNwmaQHwGvBoauvGiBgBfAc4Mu2fC3wvXeBsUi6BOyIG5VGvmVleqjir5Flgk0bKG423EXE+cH5L2sglxy3pByWvt2mw7+g82jQzWyJqwVZjeV2cPLbk9f812PejnNo0M2u1urq6irdayyvHrSZeN/bezKzmvMjU4hcgGybZfXHSzNqf4sTt3AL3upKeJfunWDO9Jr0fnFObhdSlc0f+ffHP6dy5Ix07dOCmfz/NGX/O7ow99ag92XeXTVi48DP+esODXHjNf9hjhw05+cg9+CyCBQs/44Q/3MAj416p8aewPJz865N44D/306tXb278522Lyq++6gquveYq6uo6sP32X+OY40/g0Uce5k/nnMX8+fPp1KkTxxz3C7bc6qs17H3xeMQN6+VU71Jn3qcL2G34eXw091M6dqzj3kuO5e6Hn2edNVZhtVV6sPE+pxMR9O25AgD3Pf4it93/HAAbDFmVK8/8EUP3PaOWH8Fystfe+3LA93/Ar046cVHZE48/xv333sP1N95C586dmT49m+7bo2dPzrvgIr70pZV5+eWXOHL4Yfz7vgebqtoascwH7oh4rfS9pN7A9sDrEfFkHm0W2UdzszW3OnXsQMeOHYgIhn93Ww755d+pn875/sw5ix0LsHy3LpSf7WlFttnmX+Gtt95crOz6a6/hR4cPp3PnzgD07t0bgPXWW3/RMWutNYR5n8zj008/XXScNW+ZD9ySbgP+X0SMl9SP7F78sWRpk5ERcW4e7RZVXZ145OoTWXNAX/5y7QOMGf8aa6zWl+/suhnfGrYx02Z+yHG/v4HJr78PwLd23IgRP/0WfXt1Z9///nONe29t6bVXX+WpJ8fyf386hy5dunDs8SewwYYbLXbMv+++i/XWX99Bu4VaslZJreU1r2WNkgVVDiVbHWtPYEvKTAcsXXFrwbQJOXWt/fnss2Cr7/0va33912y+weqsv2Y/unTuyLxP57Ptgb/n0hsf4S+nHLjo+Fvue5ah+57BfseO5OSffLOGPbe2tmDhQmbNmsWV11zHMcedwC+O+zmlN9lNmvQy557zR/7nlBE17GUx5bFWSV7yCtzzS17vBNwB2QqBlFmrJCJGRsTmEbF5xz7Nrmy41Jk1Zy7/GfsSu269Pm+9O5Ob73kGgH/e+wwbDOn/heMffmoya/TvQ+8ey7d1V61GVl55ZXbaeRckseFGG1FXV8fMmTMBePeddzjmv4/mjN+eyYCBA2vc0+Jx4IY3JP1U0j7ApqR1Z9OCK51yarOQ+vRcgZVW6AZA1y6d2GnLdXnx1Xe59f5n+dpXspUdt9tsCJNefw+AwQM+XzRs6Lqr0aVzR6Z/8FHbd9xqYseddmbME9nSzq++OoX58+fTs2dPZs+ezdFHDudnxxzHJptuVuNeFpNU+VZrec0qOQwYAewM7J+WNgTYCrg0pzYLaZU+K/LXEQfRoa6OujoxavRT3PngeB55ejKX/vYQfnrgMD6aO48jR1wNwD47DeX7e2zJ/AUL+WTefA468ZIafwLLy4nHH8vYMU/wwQcz2WXY9hx51E/ZZ59vc/L//JJ999qDTp06cfpv/hdJ/OPqK3n9jdcZedEFjLzoAgAu+usliy5eWvPaw0i6UmpmEarqNiZ1BfaMiOubO7bbJkd7voR9wcwxLVqLx5YRXTsu+e0z65x4V8Ux58Uzv17TKJ/7TffpET67S7qCbEnD/fNu08yspZwqASR9Dfg+sDvwBLAN2WyTLzwZx8ys1uoKNB0wr3ncbwKvkz16/vj0vMkpDtpm1l61h5F0pfJKldwArEqWFtlT0vJ4cSkza8eW+emAEfFzYA3gLGAH4EWgr6T9JK2QR5tmZkuiSDnu3C5ORua+iBhOFsS/D+wFvJpXm2ZmrVWtBylI6irpCUnPSJog6bRUvoakxyVNknStpM6pvEt6PyntH9RsX6vweZsVEfMj4taIOBAY0BZtmpm1RBVH3POAYRGxMTAU2E3SVsCZwDkRsRYwk+x+F9L/z0zl56Tjysrr4uRzlM9pb1Rmn5lZm6viw4IDmJPedkpbAMPIMg8AlwGnkk3g2Cu9huz64PmSVO5J73lNB9wjp3rNzHJRzdy1pA7Ak8BawAXAZOCDiFiQDnkTqF+AqD/wBkBELJA0C+gNTGuq/jZZjxtAUh9gernfImZmtdKSEbek4cDwkqKRETGy/k1ELASGSuoB3ASsW61+Qk45bklbSbpf0o2SNpE0HhgPvCtptzzaNDNbEi3JcZeuZJq2kY3VmdZpug/4KtBDUv1geTXgrfT6LdK1v7R/JWB6ub7mdXHyfOC3wDXAvcDhEbEK2VNwfpdTm2ZmrVZXp4q3ciT1TSPt+hVRdwEmkgXw76TDDgH+mV7fkt6T9t/bXGYirxx3x4i4G0DSiIh4DCAiXmgPk9fNzBqqYmzqB1yW8tx1wHURcZuk54F/SDoDeBq4OB1/MXCFpEnADOB7zTWQV+AufVjC3Ab7nOM2s3anWnE7Ip4FNmmk/BVgi0bKPwG+25I28grcG0uaDQjoll6T3nfNqU0zs1YrUjYgr1klHfKo18wsLwWK2/kt62pmViTL/LKuZmZFs8ynSszMisaB28ysYAoUtx24zczAI24zs8IpUNx24DYzg2LNKml2rRJJP5O0ojIXS3pK0q5t0Tkzs7ZSJ1W81Voli0z9KCJmA7sCPYGDgP/NtVdmZm2sSM+crCRVUt/N3YErImKCipTFNzOrQJHCWiWB+0lJd5M98PckSd1ZfBEpM7PCK1CKu6LAfRjZAy9fiYiPJfUGDs23W2ZmbatIFyebDNySNm1QNLhIf0qYmbWEKE58KzfiPqvMvvonFpuZLRUKNOBuOnBHxI5t2REzs1oqUkahknncy0n6taSR6f0QSXvk3zUzs7ZTpOmAlczjvhT4FNg6vX8LOCO3HpmZ1UC1bsCRNEDSfZKelzRB0s9S+bWSxqXtVUnjUvkgSXNL9v25ub5WMqtkzYjYX9IBAGlmSTv4nWNmVj1VnFWyADguIp5K06eflDQ6IvavP0DSWcCsknMmR8TQShuoJHB/mh4xH6nBNYF5lTZgZlYEVXxY8FRganr9oaSJQH/g+awdCdiPJZjgUUmq5BTgX8AASVcB9wAntLZBM7P2KI+1SiQNInvi++MlxdsB70bEyyVla0h6WtJ/JG3XXL3NjrgjYrSkp4CtyG5//1lETKu452ZmBdCSAbek4cDwkqKRETGywTErAKOAn6f1nuodAFxT8n4qMDAipkvaDLhZ0pcbnLOYSpd1/RqwLVm6pBNwU4XnmZkVQksu3aUgPbKp/ZI6kQXtqyLixpLyjsC+wGYldc0jpZ8j4klJk4G1gbFN1d9s4JZ0IbAWn/+G+LGknSPiqObONTMrimpdm0w57IuBiRFxdoPdOwMvRMSbJcf3BWZExEJJg4EhwCvl2qhkxD0MWC8i6i9OXgZMqPxjmJm1f1WcVbIN2fLXz9VP+QN+GRF3AN9j8TQJwPbACEnzyRbwOyIiZpRroJLAPQkYCLyW3g9IZWZmS41qzXKOiIdoImUeET9spGwUWVqlYuUWmbqVLKfdHZgo6Yn0fkvgiZY0YmbW3i0Va5UAf2yzXpiZ1ViR7isst8jUf9qyI2ZmtVScsF3ZIlNbSRojaY6kTyUtlNTk/EIzsyLqUKeKt1qr5OLk+WRXQq8HNgcOJptjaGa21ChSqqSSW96JiElAh4hYGBGXArvl2y0zs7ZVpGVdKxlxfyypMzBO0u/Jbs+sKOCbmRVFS9YgqbVKAvBB6bijgY/I5nHvm2enzMza2lI14o6I+htvPgFOg2xBcGD/Jk+qgt+dd1ye1VtB3Tr+7Vp3wdqh7w5ddYnrKFKOu9JFphr6alV7YWZWYx2WgcBtZrZUaQez/CpW7pb3TZvaRba0q5nZUmOpCNzAWWX2vVDtjpiZ1dJSkeOOiB3bsiNmZrW0tIy4zcyWGQUacDtwm5kBdCxQ5HbgNjOjWCPuSlYHlKQfSDo5vR8oaYv8u2Zm1nbqpIq3WqvklvcLyW64OSC9/xC4ILcemZnVQLVueZc0QNJ9kp6XNEHSz1L5qZLekjQubbuXnHOSpEmSXpT09eb6WkmqZMuI2FTS0wARMTMtOmVmttSo4qySBcBxEfGUpO7Ak5JGp33nRMRiTxeTtD7Z0tlfBlYF/i1p7YhY2FQDlQTu+ZI6kD1vsv5R8p+1/LOYmbVf1XpAQkRMJVtFlYj4UNJEoH+ZU/YC/hER84ApkiYBWwCPNnVCJamS84CbgC9J+g3wEPDbyj6CmVkx1KnyrVKSBgGbAI+noqMlPSvpEkk9U1l/4I2S096kfKBvPnBHxFXACcDvyH6L7B0R11fedTOz9k8t+Z80XNLYkm34F+qTVgBGAT+PiNnARcCawFCyWFru7vSymk2VSBoIfAzcWloWEa+3tlEzs/amJSPpiBgJjGxqv6ROZEH7qoi4MZ3zbsn+vwK3pbdvkT3noN5qqaxJleS4byfLbwvoCqwBvEiWSDczWypU6+KkskVPLgYmRsTZJeX9Uv4bYB9gfHp9C3C1pLPJLk4OAZ4o10YlD1LYsEGnNgV+UumHMDMrgiouMrUN2ZPDnpM0LpX9EjhA0lCygfCrwI8BImKCpOuA58lmpBxVbkYJtOLOyTTFZcuWnmdm1p51qNKTdCPiIbIMRUN3lDnnN8BvKm2jkhz3sSVv64BNAT8/ysyWKu3hjshKVTLi7l7yegFZzntUPt0xM6uNpWZZ13TjTfeIOL6N+mNmVhMFGnCXfXRZx4hYIGmbtuyQmVkt1DWalm6fyo24nyDLZ4+TdAtwPfBR/c76uYlmZkuDpWLEXaIrMB0YxufzuQNw4DazpUbHAiW5ywXuL6UZJeP5PGDXi1x7ZWbWxpaWEXcHYAUan4/owG1mS5WlZTrg1IgY0WY9MTOroQLF7bKBu0Afw8xsyVTpxsk2US5w79RmvTAzq7GlIlUSETPasiNmZrW0VATuakmPOiMi3s+7LTOz1ipO2M4praPMqZKmka3d/ZKk9yWdnEd7ZmZLqlpPeW8LeeXjjyFbk/YrEdErInoCWwLbSDompzbNzFpNUsVbreUVuA8CDoiIKfUFEfEK8APg4JzaNDNrtboWbLWWV467U0RMa1gYEe+nZ7GZmbUrvjgJn7Zyn5lZTbSHFEil8grcG0ua3Uh5/QOHzczalWqlQCQNAC4HViZbHmRkRPxJ0h+APckGr5OBQyPiA0mDgIlkEzkAHouII8q1kUvgjogOedRrZpaXKo64FwDHpefzdgeelDQaGA2clJ5zcCZwEnBiOmdyRAyttIG8pgMOK3m9RoN9++bRppnZklALtnIiYmpEPJVef0g2mu4fEXdHxIJ02GPAaq3ta14XSP9Y8rrh8yl/nVObZmat1kGqeKtUSoNsAjzeYNePgDtL3q8h6WlJ/5G0XXP15pXjVhOvG3tvZlZzLcmUSBoODC8pGhkRIxscswLZwPXnETG7pPxXZOmUq1LRVGBgREyXtBlws6Qvl57TUF6BO5p43dh7M7OaUwvGlClIj2xqf5r2PAq4qvQxj5J+COwB7BQRkeqaB8xLr5+UNBlYGxjbVP15Be7B6TmVKnlNer9G06eZmdVGta5NKrvKeTEwMSLOLinfDTgB+FpEfFxS3heYERELJQ0GhgCvlGsjr8C9V8nrPzbY1/C9mVnNVfEp79uQ3T3+nKRxqeyXwHlAF2B0msFSP+1ve2CEpPnAZ8ARza3Omtd0wP+Uvk9/NmwAvBUR7+XRppnZkqjWiDsiHqLxa3l3NHH8KL44iaOsvKYD/lnSl9PrlYBnyCakPy3pgDzaNDNbEnVSxVut5TUdcLuImJBeHwq8FBEbApuR5XjMzNqVOlW+1VpbrFWyC3A9QES8U6T1AMxs2dGSWSW1llfg/kDSHsBbZIn6wwAkdQS65dSmmVmrFWlMmVfg/jHZFdRVyCafv5PKdwJuz6nNQpoz433uufgPzJ39AQjW3353Ntp570X7x901ikev/ys/POdaunVfibdeeIZ/XXAa3fusAsDgTbdh8z0PrFX3LScfTHuPURf8jjmzZiLB5jvtwda7f4d/X3sJE8c+jCSWX6kn3z7yRFbs1YeI4Pa//x8vPf04nbp05dtHnsiqg9eu9ccolGV+xB0RLwG7NVJ+lyQ/hLiE6urYer//ou/qQ/j0k4+54fSfstr6m9Br1dWZM+N93nz+SVbo9aXFzuk3ZAN2/+8RNeqxtYUOHTrwjYOOZNXBazNv7sdceNKPWWujzdl2z/3Zef8fAfDonaO4b9Tl7PVfx/LSuMeZ/s5bHPOnK3nz5YnccvE5HPGbi2r8KYqlPeSuK9UmD3OQtL6k0yVNAvxtKrF8j970XX0IAJ27LkfPfgP4aOZ0AB6+9i9s9Z3DC/UnnFVH9569F42Yu3Rbjr79BzJ7xjS6Lrf8omM+/eSTRSvaTRzzMEO33xVJDFh7fT756CM+TN8jq0yRZpXk9pT3tLjKAWmbD6wObB4Rr+bVZtHNnvYO016fzMqD12HK04+yfI/e9Bkw+AvHvTN5ItedeiTL9ejN1t89nF79B7V9Z63NzHzvHaZOmcRqa60HwOh//I2nH7ibrt2W57BTzgHgw5nTWKn353+Zrdi7D7NnTKN7z9416XMR1T4cVy6vedyPkuWyOwLfjojNgA+bC9qShksaK2nsI7dck0fX2q35n8zlrgvPYJv9f4zqOvDUHf/gK3t98fGcfVdfi4POvJz9Tr2IDYd9i39d4JTJ0mzeJ3O55uyT2f2QoxaNtnf53uGccOF1bLztzjz2r5tq3MOlR5FG3HmlSt4FupM9AaJvKmt2camIGBkRm0fE5lt/a9m5T2fhggXcddHprL3VjgzebFtmvz+V2dPe4frTjuTKEw9mzsxp3HD60Xw8awaduy1Pp67ZxJzVN9qCzxYuYO6Hs2r8CSwPCxcs4JqzTmbjbXfmy1tu/4X9G2+3MxMefwCA7j37MGv65zclz54+jRV79Wmzvi4NqrUed1vI6+Lk3umOyX2BUyUNAXpI2iIinsijzaKKCO6/7Bx69BvIxrt+G4Deq63Boedcu+iYK088mG//+v/o1n0lPp41g24r9kQS777yIhFB1xVWrFX3LScRwU1//j19+6/ONnvst6h82tQ36dMvW39/4piH6dt/IADrbb41j911MxttPYw3X55Il+WWd5qkpdpDRK5QbjnuiJgFXApcKmllYD/gHEkDI2JAXu0WzTuTJvDSo/fQq/8grjvtJwBsuc8PWX2jLRo9fvKTDzHh/tuoq+tAh85d2GX4SYV6yKlV5rUXxzPuwdGsPHAw559wOAC7HHA4T957B9PefgPV1dGjz8rs9V/HALD2Jlvx0tOPc/bPfkDnzl3Y98gTy1VvjWgPKZBKKS0J23YNSqtHxGvNHXfug1O8brd9Qf/uXWrdBWuHvjt01SWOumNemVVxzPnK4JVqGuVzGXFLupXyOe1v5dGumVmrFWfAnVuqxGtum1mh+M7JButxm5m1dwVKcec2j3uIpEslnS1pNUl3Spoj6RlJm+fRppnZkijSdMC85nFfCjwKvE32WPpLgD7A8cAFObVpZtZqkireai2vwL1Cupnmj8DciLg+Ij6JiNFkz1wzM2tXpMq3WssrcH9W8np2mX1mZu1CtVIlkgZIuk/S85ImSPpZKu8labSkl9P/90zlknSepEmSnpW0aXN9zStwr5s68FzJ6/r36+TUpplZ61Uvyb0AOC4i1ge2Ao6StD7w/4B7ImIIcE96D/ANYEjahlPBCqp5TQdcL6d6zcxyUa3pgBExFZiaXn8oaSLQH9gL2CEddhlwP3BiKr88srshH5PUQ1K/VE+j8poO2OydkWZm7UlLcteShpONjuuNjIiRjRw3CNiEbJLGyiXB+B2yRfggC4cvGk4AAAr2SURBVOpvlJz2Zipr28AtaQqL3zmpkvcREWvm0a6ZWWu1JHCnIP2FQL14fVoBGEX2+MbZpbNRIiIktXpZj7xSJQ3nateRLTJ1PPB0Tm2ambVaNe+clNSJLGhfFRE3puJ361MgkvoB9evwvgWULry3WiprUi4XJyNiekRMB2YCewD3AV8FvhkR386jTTOzJVGt6YDKhtYXAxMj4uySXbcAh6TXhwD/LCk/OM0u2QqYVS6/DfmlSjoBPwKOAR4C9o6ISXm0ZWZWDVWcnr0NcBDwnKRxqeyXwP8C10k6DHiNLAsBcAewOzAJ+Bg4tLkG8kqVTCGbEnMu8DqwkaSN6neW/OlgZtY+VClyR8RDZWrbqZHjAziqJW3kFbhHp//fKG3w+QcJwIHbzNqVIj1IIa/APZ4sQJcG6/eBhyJiSk5tmpm1WnHCdo5rlZA9LHiFktebA3dK+l5ObZqZtV6BlgfM6wac0xorl9QL+DfwjzzaNTNrrWX+QQpNiYgZag9rIpqZNVCkyNSmgVvSjmRzu83M2pUCxe3c5nE/xxcfFtyL7MEKB+fRppnZkihSMiCvEfceDd4HMD0iPsqpPTOzJVKguO3VAc3MwKkSM7PiKVDkduA2M8PTAc3MCmeZz3GbmRVNnQO3mVnRFCdyO3CbmeFUiZlZ4RQobjtwm5mBR9xmZoVTpFve81qP28ysUKq5HLekSyS9J2l8Sdm1ksal7dX651FKGiRpbsm+PzdXv0fcZmZUPVXyd+B84PL6gojY//O2dBYwq+T4yRExtNLKHbjNzKjunZMR8YCkQY22k+Vk9gOGtbZ+p0rMzKBFuRJJwyWNLdmGt6Cl7YB3I+LlkrI1JD0t6T+StmuuAo+4zcxo2XTAiBgJjGxlUwcA15S8nwoMjIjpkjYDbpb05YiY3VQFDtxmZkBdG8wqkdQR2BfYrL4sIuYB89LrJyVNBtYGxjZVjwO3mRltNo97Z+CFiHjz83bVF5gREQslDQaGAK+Uq8Q5bjOzKpN0DfAosI6kNyUdlnZ9j8XTJADbA8+m6YE3AEdExIxy9XvEbWZGdUfcEXFAE+U/bKRsFDCqJfU7cJuZ4QcpmJkVToHueHfgNjMDB24zs8JxqsTMrGA84jYzK5gCxW0HbjMzoFCR24HbzIy2ueW9WhQRte6DNUPS8LSojdki/l4su3zLezG0ZMlIW3b4e7GMcuA2MysYB24zs4Jx4C4G5zGtMf5eLKN8cdLMrGA84jYzKxgHbjOzgnHgzpmkhZLGSXpG0lOStk7lgySNb3DsuZLeklRXUraypNvS+c9LuqPk/Lmp7vrt4LTvVUnPpe15SWdI6tqWn3tZ18TP91RJx0v6e/o5d0nlfSS9WnJe/c/1GUmPSFon7dtB0m0N6rxZ0mMNytaRdH+qY6KkkSXnz2rwndk57av/nk5I7R5X+j209sV3TuZvbkQMBZD0deB3wNcaHpT+I9kHeCPtvy/tGgGMjog/peM2Kjltcn3djdgxIqZJWoHsItZfgEOq8HmsOhYCPwIuamTf5JLvzI+BX9LIz05SD7KHzs6RNDgi6p9TeB5wTkT8Mx23YclpD0bEHo20Wfo9/RJwNbAicEprPpzly79R29aKwMwm9u0ATCD7D7n0sUf9gEUPFo2IZ1vSYETMAY4A9pbUqyXnWq7OBY5JT/0up9x3Zl/gVuAfZM8yrNfwO/NcSzoWEe+R3dxztFSg+8CXIQ7c+euW/gR9AfgbcHoTxx1A9hDRm4BvSuqUyi8ALpZ0n6RfSVq15Jw1G/zZu11jFUfEbGAK2dOjrX14HXgIOKiRffU/18nAscDZTdRR/525hsV/2Z8D3CvpTknHpJF5ve0afGfWbKziNHrvAHypZR/L2oIDd/7mRsTQiFgX2A24vOEoRlJnYHfg5hRkHwe+DhARdwGDgb8C6wJPS+qbTp2c6q7fHizTD4+c2lZT82xLy38H/IIv/ndY/3NdE/g5jczXlrQy2S/ihyLiJWC+pA0AIuJSYD3gerK/5B6rz6eTpUpKvzOTW/fxrJYcuNtQRDwK9AH6Ntj1daAH8Fy6SLUtJSOoiJgREVdHxEHAGGD7lrQrqTswCHip1Z23lpoO9GxQ1guYVv8mIl4GxgH7lannFhr/ee+X6p+SvjODWPw783ZEXBIRewELgA1a0nlJg8ny8O+15DxrGw7cbUjSumR/fk5vsOsA4PCIGBQRg4A1gF0kLSdpmKTl0vndgTXJ/syutM0VgAvJRvNN5UqtytK1hamShgGk6wu7kaVHSv0GOL5MVdsCjY2KDwB2K/nObEbKc0varT7VJmkVoDfwVqV9T3/R/Rk4P3yHXrvkWSX56yZpXHot4JCIWFifLUlBeTeyC4gARMRHkh4C9gQGAudLWkD2i/ZvETFG0iBSLrSkrUsi4rz0+r6Ukqkjy5s3lVu3/BwMXCCpPkd9WkRMLs2URcQESU8Bm5acV/9zFfApcHhppelnvzqwaBpgRExJU/22BHYF/iTpk7T7FxHxTho4bNfgO3NGRNzA59/TTmQj9CtoOrduNeZb3s3MCsapEjOzgnHgNjMrGAduM7OCceA2MysYB24zs4Jx4LbFlKwSN17S9fVzyFtZ198lfSe9/puk9cscu4PSyoktbONVSX0qLW+ijh9KOr8a7Zq1BQdua6j+Fv0NyOYQH1G6s4JFkRoVEYdHxPNlDtkBaHHgNlsWOXBbOQ8Ca6XR8IOSbgGel9RB0h8kjZH0bFp6FGXOl/SipH9TskBRWh968/R6N2Vrkz8j6Z50Q8kRZKvljZO0naS+kkalNsZI2iad21vS3crWjf4bLViDRdIWkh6V9LRK1rlOBqQ+vizplJJzfiDpidSvv0jq0KDO5SXdnj7LeEn7t/Df2KzFfOekNSqNrL8B/CsVbQpskO7QGw7MioivpMWLHpZ0N7AJsA6wPrAy8DxwSYN6+5ItmLV9qqtXRMyQ9GdgTkT8MR13Ndma0g9JGgjcRbZw0ilkCyuNkPRN4LAWfKwXgO0iYoGyBwj8Fvh22rcF2XoeHwNjJN0OfATsD2wTEfMlXQgcCFxeUuduwNsR8c3U75Va0B+zVnHgtoZKb9F/ELiYLIXxRERMSeW7AhvV56+BlchWqtseuCYiFgJvS7q3kfq3Ah6orysiZjTRj52B9UtuD18xrbuyPdk61ETE7ZJasv7KSsBlkoaQrdLXqWTf6IiYDiDpRrI1QhaQrQEyJvWjG19cdOk54CxJZwK3NbNCo1lVOHBbQ4uehFIvBa2PSouAn6YlZ0uP272K/agDtoqIT0oLtWTr+p8O3BcR+6T0zP0l+xqu/RBkn/OyiDipqQoj4iVJm5Ity3uGpHsiYsSSdNKsOc5xW2vcBRxZsgLd2pKWBx4A9k858H7Ajo2c+xiwvaQ10rn1T+X5EOhectzdwE/r30iq/2XyAPD9VPYNvrh0ajkr8fkqeT9ssG8XSb0kdQP2Bh4G7gG+o+xRXqT9q5eepOzBFh9HxJXAH1h8sSizXHjEba3xN7L1n59SNgR+nyzY3QQMI8ttvw482vDEiHg/5chvVPaczfeAXcgewXWDpL3IAvZ/k62s9yzZ9/QBsguYpwHXSJoAPEL5JW6flfRZen0d8HuyVMmvgdsbHPsEMApYDbgyIsYCpGPvTn2dDxwFvFZy3obAH1I784Ejy/THrCq8OqCZWcE4VWJmVjAO3GZmBePAbWZWMA7cZmYF48BtZlYwDtxmZgXjwG1mVjD/HweE5n9t9k3YAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["mP0P1_WTBRwZ","AlX3tF90Mx2-","7lAASlTuBVCu","EGk80wOeBXja","UMY_xtSNzaK9","VLAyQr3jBwMK","W_diGAld7E0G","Qpg0Fayj8x33","yZlEe6OmL4mR"],"provenance":[],"authorship_tag":"ABX9TyNfPwEm5KjglpkPvIrUo7j9"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}